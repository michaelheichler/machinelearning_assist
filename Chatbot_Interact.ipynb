{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# AI Chatbot for Matrix\n",
    "# Sourcecode from Huggingface and Matrix NIO\n",
    "# Customization / Coding: Philip Ehnert\n",
    "# Optimization / Docker: Michael Heichler\n",
    "# 2020 under MIT License\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build / Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Package Nightly (not a must, Standard is also fine) for Windows\n",
    "# pip install numpy\n",
    "# pip install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html\n",
    "\n",
    "# Transformers Package from Source (recommended)\n",
    "# git clone https://github.com/huggingface/transformers.git\n",
    "# cd transformers\n",
    "# pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import sentencepiece\n",
    "import warnings\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Matrix NIO Packages\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from itertools import chain\n",
    "from pprint import pformat\n",
    "\n",
    "from transformers import cached_path\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "from nio import AsyncClient, MatrixRoom, RoomMessageText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init of NIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = \"credentials.json\"\n",
    "\n",
    "def write_details_to_disk(resp: LoginResponse, homeserver) -> None:\n",
    "    \"\"\"Writes the required login details to disk so we can log in later without\n",
    "    using a password.\n",
    "\n",
    "    Arguments:\n",
    "        resp {LoginResponse} -- the successful client login response.\n",
    "        homeserver -- URL of homeserver, e.g. \"https://matrix.example.org\"\n",
    "    \"\"\"\n",
    "    # open the config file in write-mode\n",
    "    with open(CONFIG_FILE, \"w\") as f:\n",
    "        # write the login details to disk\n",
    "        json.dump(\n",
    "            {\n",
    "                \"homeserver\": homeserver,  # e.g. \"https://matrix.example.org\"\n",
    "                \"user_id\": resp.user_id,  # e.g. \"@user:example.org\"\n",
    "                \"device_id\": resp.device_id,  # device ID, 10 uppercase letters\n",
    "                \"access_token\": resp.access_token  # cryptogr. access token\n",
    "            },\n",
    "            f\n",
    "        )\n",
    "\n",
    "async def matrix() -> None:\n",
    "    # If there are no previously-saved credentials, we'll use the password\n",
    "    if not os.path.exists(CONFIG_FILE):\n",
    "        print(\"First time use. Did not find credential file. Asking for \"\n",
    "              \"homeserver, user, and password to create credential file.\")\n",
    "        homeserver = \"https://matrix.example.org\"\n",
    "        homeserver = input(f\"Enter your homeserver URL: [{homeserver}] \")\n",
    "\n",
    "        if not (homeserver.startswith(\"https://\")\n",
    "                or homeserver.startswith(\"http://\")):\n",
    "            homeserver = \"https://\" + homeserver\n",
    "\n",
    "        user_id = \"@user:example.org\"\n",
    "        user_id = input(f\"Enter your full user ID: [{user_id}] \")\n",
    "\n",
    "        device_name = \"matrix-nio\"\n",
    "        device_name = input(f\"Choose a name for this device: [{device_name}] \")\n",
    "\n",
    "        client = AsyncClient(homeserver, user_id)\n",
    "        pw = getpass.getpass()\n",
    "\n",
    "        resp = await client.login(pw, device_name=device_name)\n",
    "\n",
    "        # check that we logged in succesfully\n",
    "        if (isinstance(resp, LoginResponse)):\n",
    "            write_details_to_disk(resp, homeserver)\n",
    "        else:\n",
    "            print(f\"homeserver = \\\"{homeserver}\\\"; user = \\\"{user_id}\\\"\")\n",
    "            print(f\"Failed to log in: {resp}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        print(\n",
    "            \"Logged in using a password. Credentials were stored.\",\n",
    "            \"Try running the script again to login with credentials.\"\n",
    "        )\n",
    "\n",
    "    # Otherwise the config file exists, so we'll use the stored credentials\n",
    "    else:\n",
    "        # open the file in read-only mode\n",
    "        with open(CONFIG_FILE, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "            client = AsyncClient(config['homeserver'])\n",
    "\n",
    "            client.access_token = config['access_token']\n",
    "            client.user_id = config['user_id']\n",
    "            client.device_id = config['device_id']\n",
    "\n",
    "        # Now we can send messages as the user\n",
    "        room_id = \"!myfavouriteroomid:example.org\"\n",
    "        room_id = input(f\"Enter room id: [{room_id}] \")\n",
    "        \n",
    "        history.append(tokenizer.encode(raw_text))\n",
    "        with torch.no_grad():\n",
    "            out_ids = sample_sequence(personality, history, tokenizer, model, args)\n",
    "        if \n",
    "        await client.room_send(\n",
    "            room_id,\n",
    "            message_type=\"m.room.message\",\n",
    "            content={\n",
    "                \"msgtype\": \"m.text\",\n",
    "                \"body\": \"Hello world!\"\n",
    "            }\n",
    "        )\n",
    "        print(\"Logged in using stored credentials. Sent a test message.\")\n",
    "\n",
    "    # Either way we're logged in here, too\n",
    "    await client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxid = \"\"  # Bot's username\n",
    "password = \"mJBWq9nTdYZsYg\"  # Bot's password\n",
    "hs_url = \"https://matrix.mheichler.de\"\n",
    "room_id = \"!WCkuBFwvWVlGLsAgnu:matrix.mheichler.de\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ['<speaker1>', '<speaker2>']}\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "    if num_added_tokens > 0:\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
    "        \n",
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, dataset_path, dataset_cache):\n",
    "    \"\"\" Get tokenized PERSONACHAT dataset from S3 or cache.\"\"\"\n",
    "    dataset_path = dataset_path\n",
    "    dataset_cache = dataset_cache + '_' + type(tokenizer).__name__  # To avoid using GPT cache for GPT-2 and vice-versa\n",
    "    if dataset_cache and os.path.isfile(dataset_cache):\n",
    "        logger.info(\"Load tokenized dataset from cache at %s\", dataset_cache)\n",
    "        dataset = torch.load(dataset_cache)\n",
    "    else:\n",
    "        logger.info(\"Download dataset from %s\", dataset_path)\n",
    "        personachat_file = cached_path(dataset_path)\n",
    "        with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            dataset = json.loads(f.read())\n",
    "\n",
    "        logger.info(\"Tokenize and encode the dataset\")\n",
    "        def tokenize(obj):\n",
    "            if isinstance(obj, str):\n",
    "                return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "            if isinstance(obj, dict):\n",
    "                return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "            return list(tokenize(o) for o in obj)\n",
    "        dataset = tokenize(dataset)\n",
    "        torch.save(dataset, dataset_cache)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "            threshold: a minimal threshold to keep logits\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(personality, history, tokenizer, model, args, current_output=None):\n",
    "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "    if current_output is None:\n",
    "        current_output = []\n",
    "\n",
    "    for i in range(args.max_length):\n",
    "        instance = build_input_from_segments(personality, history, current_output, tokenizer, with_eos=False)\n",
    "\n",
    "        input_ids = torch.tensor(instance[\"input_ids\"], device=args.device).unsqueeze(0)\n",
    "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=args.device).unsqueeze(0)\n",
    "\n",
    "        logits = model(input_ids, token_type_ids=token_type_ids)\n",
    "        if isinstance(logits, tuple):  # for gpt2 and maybe others\n",
    "            logits = logits[0]\n",
    "        logits = logits[0, -1, :] / args.temperature\n",
    "        logits = top_filtering(logits, top_k=args.top_k, top_p=args.top_p)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        prev = torch.topk(probs, 1)[1] if args.no_sample else torch.multinomial(probs, 1)\n",
    "        if i < args.min_length and prev.item() in special_tokens_ids:\n",
    "            while prev.item() in special_tokens_ids:\n",
    "                if probs.max().item() == 1:\n",
    "                    warnings.warn(\"Warning: model generating special token with probability 1.\")\n",
    "                    break  # avoid infinitely looping over special token\n",
    "                prev = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if prev.item() in special_tokens_ids:\n",
    "            break\n",
    "        current_output.append(prev.item())\n",
    "\n",
    "    return current_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exec Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--dataset_path\", type=str, default=\"\", help=\"Path or url of the dataset. If empty download from S3.\")\n",
    "    parser.add_argument(\"--dataset_cache\", type=str, default='./dataset_cache', help=\"Path or url of the dataset cache\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"openai-gpt\", help=\"Model type (openai-gpt or gpt2)\", choices=['openai-gpt', 'gpt2'])  # anything besides gpt2 will load openai-gpt\n",
    "    parser.add_argument(\"--model_checkpoint\", type=str, default=\"\", help=\"Path, url or short name of the model\")\n",
    "    parser.add_argument(\"--max_history\", type=int, default=2, help=\"Number of previous utterances to keep in history\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"Device (cuda or cpu)\")\n",
    "\n",
    "    parser.add_argument(\"--no_sample\", action='store_true', help=\"Set to use greedy decoding instead of sampling\")\n",
    "    parser.add_argument(\"--max_length\", type=int, default=20, help=\"Maximum length of the output utterances\")\n",
    "    parser.add_argument(\"--min_length\", type=int, default=1, help=\"Minimum length of the output utterances\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Seed\")\n",
    "    parser.add_argument(\"--temperature\", type=float, default=0.7, help=\"Sampling softmax temperature\")\n",
    "    parser.add_argument(\"--top_k\", type=int, default=0, help=\"Filter top-k tokens before sampling (<=0: no filtering)\")\n",
    "    parser.add_argument(\"--top_p\", type=float, default=0.9, help=\"Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\")\n",
    "    \n",
    "    # Arguments can be send to the file via parse_args([\"command\", \"value\"]) whereas every char is important\n",
    "    args = parser.parse_args([\"--dataset_path\", \"\"])\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__file__)\n",
    "    logger.info(pformat(args))\n",
    "    \n",
    "    if args.model_checkpoint == \"\":\n",
    "        if args.model == 'gpt2':\n",
    "            raise ValueError(\"Interacting with GPT2 requires passing a finetuned model_checkpoint\")\n",
    "        else:\n",
    "            args.model_checkpoint = download_pretrained_model()\n",
    "    \n",
    "    logger.info(\"Get pretrained model and tokenizer\")\n",
    "    tokenizer_class, model_class = (GPT2Tokenizer, GPT2LMHeadModel) if args.model == 'gpt2' else (OpenAIGPTTokenizer, OpenAIGPTLMHeadModel)\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.model_checkpoint)\n",
    "    model = model_class.from_pretrained(args.model_checkpoint)\n",
    "    model.to(args.device)\n",
    "    add_special_tokens_(model, tokenizer)\n",
    "\n",
    "    logger.info(\"Sample a personality\")\n",
    "    dataset = get_dataset(tokenizer, args.dataset_path, args.dataset_cache)\n",
    "    personalities = [dialog[\"personality\"] for dataset in dataset.values() for dialog in dataset]\n",
    "    personality = random.choice(personalities)\n",
    "    logger.info(\"Selected personality: %s\", tokenizer.decode(chain(*personality)))\n",
    "\n",
    "    history = []\n",
    "    while True:\n",
    "        raw_text = input(\">>> \")\n",
    "        while not raw_text:\n",
    "            print('Prompt should not be empty!')\n",
    "            raw_text = input(\">>> \")\n",
    "        \n",
    "        history.append(out_ids)\n",
    "        history = history[-(2*args.max_history+1):]\n",
    "        out_text = n\n",
    "        print(out_text)       \n",
    "    \n",
    "    asyncio.get_event_loop().run_until_complete(matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
